{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure LangChain and LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open AI Key  is loaded\n",
      "Langsmith Key is loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Read the file and load the keys\n",
    "token_file = open('../token.txt', 'r')\n",
    "# Read the first line\n",
    "open_ai_token_line = token_file.readline()\n",
    "\n",
    "if open_ai_token_line:\n",
    "    open_ai_key = open_ai_token_line.split('=')[1].strip()\n",
    "    print(f'Open AI Key  is loaded')\n",
    "\n",
    "# Read the second line\n",
    "langsmith_token_line = token_file.readline()\n",
    "\n",
    "if langsmith_token_line:\n",
    "    langsmith_key = langsmith_token_line.split('=')[1].strip()\n",
    "    print(f'Langsmith Key is loaded')\n",
    "\n",
    "\n",
    "if langsmith_key is not None:\n",
    "    # Configure LangSmith\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = \"MULTI-AGENT\"\n",
    "    os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = langsmith_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ecore parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "\n",
    "#PyEcore\n",
    "from pyecore.resources import ResourceSet, URI\n",
    "\n",
    "class EcoreParser:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.resource_set = ResourceSet()\n",
    "        self.classes_per_metamodel = {}\n",
    "    \n",
    "    def check_ecore_class(self, ecore_name: str, class_to_test: str):\n",
    "        ecore_path = f'../temp/{ecore_name}.ecore'\n",
    "        if ecore_path not in self.classes_per_metamodel:\n",
    "            self.classes_per_metamodel[ecore_path] = []\n",
    "        if class_to_test in self.classes_per_metamodel[ecore_path]:\n",
    "            return True\n",
    "        #register metamodel\n",
    "        resource_path = self.resource_set.get_resource(URI(ecore_path))\n",
    "        content = resource_path.contents[0]\n",
    "        if content is None:\n",
    "            return False\n",
    "        if content.nsURI is None:\n",
    "            return False\n",
    "        if  content.nsURI not in self.resource_set.metamodel_registry:\n",
    "            self.resource_set.metamodel_registry[content.nsURI] = content\n",
    "        #check class\n",
    "        for element in resource_path.contents.contents:    \n",
    "            class_name = element.eClass.name\n",
    "            if class_name == class_to_test:\n",
    "                if self.classes_per_metamodel[ecore_path] is None:\n",
    "                    self.classes_per_metamodel[ecore_path].append(class_name)\n",
    "                return True        \n",
    "        return False\n",
    "    \n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "from langchain.agents import tool\n",
    "\n",
    "class CheckInput(BaseModel):\n",
    "    metamodel_name: str = Field(description=\"Name of the metamodel to be checked\")\n",
    "    class_to_test: str = Field(description=\"Class to be tested  and check if it exists in the metamodel\")\n",
    "\n",
    "class CheckEcoreClassTool():\n",
    "    \"\"\"\n",
    "    CheckEcoreClassTool class.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    @tool(\"check_ecore_class\", args_schema=CheckInput)\n",
    "    def check_ecore_class(metamodel_name: str, class_to_test: str) -> bool:\n",
    "        \"\"\"\n",
    "        Run the tool synchronously.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        metamodel_name : str\n",
    "            The path to the Ecore file.\n",
    "        class_to_test : str\n",
    "            The class to test.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if the Ecore class is found, False otherwise.\n",
    "        \"\"\"\n",
    "        parser = EcoreParser()\n",
    "        return parser.check_ecore_class(metamodel_name, class_to_test)\n",
    "    \n",
    "    def get_tool(self) -> Tool:\n",
    "        \"\"\"\n",
    "        Get the tool.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tool\n",
    "            The tool.\n",
    "        \"\"\"\n",
    "        return Tool(\n",
    "            func=CheckEcoreClassTool.check_ecore_class,\n",
    "            name=\"check_ecore_class\",\n",
    "            description=\"Useful for when you need to verify if a given class exists in a specific metamodel\",\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PlantUML Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.agents import Tool\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "class MetamodelReaderToolCreator:\n",
    "\n",
    "    def create_reader_tool(self, document=None):\n",
    "        \"\"\"\n",
    "        Create reader tool for the given document.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        document : list[dict], optional\n",
    "            A dictionary representing the document. It should have a 'name' key\n",
    "            representing the name of the document and a 'content' key representing the content of the document,\n",
    "            by default None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tool\n",
    "            A Tool object representing the retrieval tool created for the document.\n",
    "        \"\"\"  \n",
    "\n",
    "        return Tool(\n",
    "                name=f'{document[\"name\"]}_Reader',\n",
    "                description=f\"useful when you want to analyze or get attributes from the PlantUML metamodel {document['name']}\",\n",
    "                func=(lambda _: document['content'].strip()),\n",
    "        )\n",
    "    \n",
    "reader_tools_creator = MetamodelReaderToolCreator()\n",
    "\n",
    "meta1_rel_path = '../temp/Book.txt'\n",
    "meta1_file = open(meta1_rel_path, 'r')\n",
    "meta2_rel_path = '../temp/Publication.txt'\n",
    "meta2_file = open(meta2_rel_path, 'r')\n",
    "\n",
    "document1 = {\"name\": f\"{meta1_rel_path.split('/')[-1].split('.')[0]}\", \"content\": meta1_file.read()}\n",
    "document2 = {\"name\": f\"{meta2_rel_path.split('/')[-1].split('.')[0]}\", \"content\": meta2_file.read()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tools for all agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tool(name='check_ecore_class', description='Useful for when you need to verify if a given class exists in a specific metamodel', func=StructuredTool(name='check_ecore_class', description='check_ecore_class(metamodel_name: str, class_to_test: str) -> bool - Run the tool synchronously.\\n\\n        Parameters\\n        ----------\\n        metamodel_name : str\\n            The path to the Ecore file.\\n        class_to_test : str\\n            The class to test.\\n\\n        Returns\\n        -------\\n        bool\\n            True if the Ecore class is found, False otherwise.', args_schema=<class '__main__.CheckInput'>, func=<function CheckEcoreClassTool.check_ecore_class at 0x000001C6CAF30FE0>)),\n",
       " Tool(name='Book_Reader', description='useful when you want to analyze or get attributes from the PlantUML metamodel Book', func=<function MetamodelReaderToolCreator.create_reader_tool.<locals>.<lambda> at 0x000001C6B9373F60>),\n",
       " Tool(name='Publication_Reader', description='useful when you want to analyze or get attributes from the PlantUML metamodel Publication', func=<function MetamodelReaderToolCreator.create_reader_tool.<locals>.<lambda> at 0x000001C6B9373EC0>)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkEcoreClassTool = CheckEcoreClassTool()\n",
    "tools = [checkEcoreClassTool.get_tool()]\n",
    "tools.append(reader_tools_creator.create_reader_tool(document1))\n",
    "tools.append(reader_tools_creator.create_reader_tool(document2))\n",
    "\n",
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan and execute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nGiven the user inputs, create a plan to solve it. Each plan should comprise an action from the following {num_tools} types:\\n\\n{tool_descriptions}\\n\\n{num_tools}. join(): Collects and combines results from prior actions.\\n\\n - An LLM agent is called upon invoking join() to either finalize the user query or wait until the plans are executed.\\n\\n - join should always be the last action in the plan, and will be called in two scenarios:\\n\\n   (a) if the answer can be determined by gathering the outputs from tasks to generate the final response.\\n\\n   (b) if the answer cannot be determined in the planning phase before you execute the plans. Guidelines:\\n\\n - Each action described above contains input/output types and description.\\n\\n    - You must strictly adhere to the input and output types for each action.\\n\\n    - The action descriptions contain the guidelines. You MUST strictly follow those guidelines when you use the actions.\\n\\n - Each action in the plan should strictly be one of the above types. Follow the Python conventions for each action.\\n\\n - Each action MUST have a unique ID, which is strictly increasing.\\n\\n - Inputs for actions can either be constants or outputs from preceding actions. In the latter case, use the format $id to denote the ID of the previous action whose output will be the input.\\n\\n - Always call join as the last action in the plan. Say '<END_OF_PLAN>' after you call join\\n\\n - Ensure the plan maximizes parallelizability.\\n\\n - Only use the provided action types. If a query cannot be addressed using these, invoke the join action for the next steps.\\n\\n - Never introduce new actions other than the ones provided.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the base template\n",
    "template = \"\"\"\n",
    "Given the user inputs, create a plan to solve it. Each plan should comprise an action from the following {num_tools} types:\n",
    "\n",
    "{tool_descriptions}\n",
    "\n",
    "{num_tools}. join(): Collects and combines results from prior actions.\n",
    "\n",
    " - An LLM agent is called upon invoking join() to either finalize the user query or wait until the plans are executed.\n",
    "\n",
    " - join should always be the last action in the plan, and will be called in two scenarios:\n",
    "\n",
    "   (a) if the answer can be determined by gathering the outputs from tasks to generate the final response.\n",
    "\n",
    "   (b) if the answer cannot be determined in the planning phase before you execute the plans. Guidelines:\n",
    "\n",
    " - Each action described above contains input/output types and description.\n",
    "\n",
    "    - You must strictly adhere to the input and output types for each action.\n",
    "\n",
    "    - The action descriptions contain the guidelines. You MUST strictly follow those guidelines when you use the actions.\n",
    "\n",
    " - Each action in the plan should strictly be one of the above types. Follow the Python conventions for each action.\n",
    "\n",
    " - Each action MUST have a unique ID, which is strictly increasing.\n",
    "\n",
    " - Inputs for actions can either be constants or outputs from preceding actions. In the latter case, use the format $id to denote the ID of the previous action whose output will be the input.\n",
    "\n",
    " - Always call join as the last action in the plan. Say '<END_OF_PLAN>' after you call join\n",
    "\n",
    " - Ensure the plan maximizes parallelizability.\n",
    "\n",
    " - Only use the provided action types. If a query cannot be addressed using these, invoke the join action for the next steps.\n",
    "\n",
    " - Never introduce new actions other than the ones provided.\"\"\"\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given the user inputs, create a plan to solve it. Each plan should comprise an action from the following \u001b[33;1m\u001b[1;3m{num_tools}\u001b[0m types:\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{tool_descriptions}\u001b[0m\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{num_tools}\u001b[0m. join(): Collects and combines results from prior actions.\n",
      "\n",
      " - An LLM agent is called upon invoking join() to either finalize the user query or wait until the plans are executed.\n",
      "\n",
      " - join should always be the last action in the plan, and will be called in two scenarios:\n",
      "\n",
      "   (a) if the answer can be determined by gathering the outputs from tasks to generate the final response.\n",
      "\n",
      "   (b) if the answer cannot be determined in the planning phase before you execute the plans. Guidelines:\n",
      "\n",
      " - Each action described above contains input/output types and description.\n",
      "\n",
      "    - You must strictly adhere to the input and output types for each action.\n",
      "\n",
      "    - The action descriptions contain the guidelines. You MUST strictly follow those guidelines when you use the actions.\n",
      "\n",
      " - Each action in the plan should strictly be one of the above types. Follow the Python conventions for each action.\n",
      "\n",
      " - Each action MUST have a unique ID, which is strictly increasing.\n",
      "\n",
      " - Inputs for actions can either be constants or outputs from preceding actions. In the latter case, use the format $id to denote the ID of the previous action whose output will be the input.\n",
      "\n",
      " - Always call join as the last action in the plan. Say '<END_OF_PLAN>' after you call join\n",
      "\n",
      " - Ensure the plan maximizes parallelizability.\n",
      "\n",
      " - Only use the provided action types. If a query cannot be addressed using these, invoke the join action for the next steps.\n",
      "\n",
      " - Never introduce new actions other than the ones provided.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "print(prompt.pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "def create_agent(llm: ChatOpenAI, tools: list, system_prompt: str):\n",
    "    # Each worker node will be given a name and some tools.\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                system_prompt,\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "        ]\n",
    "    )\n",
    "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools)\n",
    "    return executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "members = [\"Researcher\", \"Coder\"]\n",
    "system_prompt = (\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following workers:  {members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH.\"\n",
    ")\n",
    "# Our team supervisor is an LLM node. It just picks the next agent to process\n",
    "# and decides when the work is completed\n",
    "options = [\"FINISH\"] + members\n",
    "# Using openai function calling can make output parsing easier for us\n",
    "function_def = {\n",
    "    \"name\": \"route\",\n",
    "    \"description\": \"Select the next role.\",\n",
    "    \"parameters\": {\n",
    "        \"title\": \"routeSchema\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"next\": {\n",
    "                \"title\": \"Next\",\n",
    "                \"anyOf\": [\n",
    "                    {\"enum\": options},\n",
    "                ],\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"next\"],\n",
    "    },\n",
    "}\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next?\"\n",
    "            \" Or should we FINISH? Select one of: {options}\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(options=str(options), members=\", \".join(members))\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=open_ai_key)\n",
    "\n",
    "supervisor_chain = (\n",
    "    prompt\n",
    "    | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
    "    | JsonOutputFunctionsParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, Any, Dict, List, Optional, Sequence, TypedDict\n",
    "import functools\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "\n",
    "# The agent state is the input to each node in the graph\n",
    "class AgentState(TypedDict):\n",
    "    # The annotation tells the graph that new messages will always\n",
    "    # be added to the current states\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    # The 'next' field indicates where to route to next\n",
    "    next: str\n",
    "\n",
    "\n",
    "research_agent = create_agent(llm, [tavily_tool], \"You are a web researcher.\")\n",
    "research_node = functools.partial(agent_node, agent=research_agent, name=\"Researcher\")\n",
    "\n",
    "# NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION. PROCEED WITH CAUTION\n",
    "code_agent = create_agent(\n",
    "    llm,\n",
    "    [python_repl_tool],\n",
    "    \"You may generate safe python code to analyze data and generate charts using matplotlib.\",\n",
    ")\n",
    "code_node = functools.partial(agent_node, agent=code_agent, name=\"Coder\")\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"Researcher\", research_node)\n",
    "workflow.add_node(\"Coder\", code_node)\n",
    "workflow.add_node(\"supervisor\", supervisor_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Researcher': 'Researcher', 'Coder': 'Coder', 'FINISH': '__end__'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for member in members:\n",
    "    # We want our workers to ALWAYS \"report back\" to the supervisor when done\n",
    "    workflow.add_edge(member, \"supervisor\")\n",
    "# The supervisor populates the \"next\" field in the graph state\n",
    "# which routes to a node or finishes\n",
    "conditional_map = {k: k for k in members}\n",
    "conditional_map[\"FINISH\"] = END\n",
    "workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n",
    "# Finally, add entrypoint\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "\n",
    "graph = workflow.compile()\n",
    "conditional_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervisor': {'next': 'Coder'}}\n",
      "----\n",
      "{'Coder': {'messages': [HumanMessage(content='The code \"Hello, World!\" has been printed to the terminal.', name='Coder')]}}\n",
      "----\n",
      "{'supervisor': {'next': 'FINISH'}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for s in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Code hello world and print it to the terminal\")\n",
    "        ]\n",
    "    }\n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
